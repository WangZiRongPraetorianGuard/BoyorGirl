{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "B_QiqBOzvGra"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>star_sign</th>\n",
       "      <th>phone_os</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>sleepiness</th>\n",
       "      <th>iq</th>\n",
       "      <th>fb_friends</th>\n",
       "      <th>yt</th>\n",
       "      <th>self_intro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>è™•å¥³åº§</td>\n",
       "      <td>Apple</td>\n",
       "      <td>154.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>583.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Beautiful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>è™•å¥³åº§</td>\n",
       "      <td>NaN</td>\n",
       "      <td>156.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Enjoying being who I'm notsss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>å°„æ‰‹åº§</td>\n",
       "      <td>Android</td>\n",
       "      <td>170.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Practice Makes perfect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>å°„æ‰‹åº§</td>\n",
       "      <td>Apple</td>\n",
       "      <td>170.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Straightforward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>å°„æ‰‹åº§</td>\n",
       "      <td>Android</td>\n",
       "      <td>158.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Humorous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>419</td>\n",
       "      <td>1</td>\n",
       "      <td>è™•å¥³åº§</td>\n",
       "      <td>Android</td>\n",
       "      <td>166.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>I hope i am a super hero.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>420</td>\n",
       "      <td>1</td>\n",
       "      <td>ç‰¡ç¾Šåº§</td>\n",
       "      <td>Android</td>\n",
       "      <td>176.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>2</td>\n",
       "      <td>God damn dope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>421</td>\n",
       "      <td>1</td>\n",
       "      <td>å¤©ç§¤åº§</td>\n",
       "      <td>Apple</td>\n",
       "      <td>174.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sunny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>422</td>\n",
       "      <td>2</td>\n",
       "      <td>å¤©è åº§</td>\n",
       "      <td>Apple</td>\n",
       "      <td>167.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>483.0</td>\n",
       "      <td>10</td>\n",
       "      <td>Light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>423</td>\n",
       "      <td>1</td>\n",
       "      <td>é›™é­šåº§</td>\n",
       "      <td>Android</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>super 666666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>423 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  gender star_sign phone_os  height  weight  sleepiness     iq  \\\n",
       "0      1       2       è™•å¥³åº§    Apple   154.0    43.0         2.0  180.0   \n",
       "1      2       2       è™•å¥³åº§      NaN   156.0    47.0         2.0  130.0   \n",
       "2      3       1       å°„æ‰‹åº§  Android   170.0    61.0         3.0   90.0   \n",
       "3      4       1       å°„æ‰‹åº§    Apple   170.0    62.0         4.0  100.0   \n",
       "4      5       2       å°„æ‰‹åº§  Android   158.0     NaN         3.0    NaN   \n",
       "..   ...     ...       ...      ...     ...     ...         ...    ...   \n",
       "418  419       1       è™•å¥³åº§  Android   166.0    66.0         4.0   90.0   \n",
       "419  420       1       ç‰¡ç¾Šåº§  Android   176.0    65.0         4.0   87.0   \n",
       "420  421       1       å¤©ç§¤åº§    Apple   174.0    72.0         NaN    NaN   \n",
       "421  422       2       å¤©è åº§    Apple   167.0    50.0         3.0  180.0   \n",
       "422  423       1       é›™é­šåº§  Android     NaN    68.0         3.0   66.0   \n",
       "\n",
       "     fb_friends   yt                     self_intro  \n",
       "0         583.0    0                      Beautiful  \n",
       "1         400.0  3.5  Enjoying being who I'm notsss  \n",
       "2         540.0    5         Practice Makes perfect  \n",
       "3         173.0    5                Straightforward  \n",
       "4           NaN  1.2                       Humorous  \n",
       "..          ...  ...                            ...  \n",
       "418      1000.0    1      I hope i am a super hero.  \n",
       "419      1300.0    2                  God damn dope  \n",
       "420         NaN  NaN                          Sunny  \n",
       "421       483.0   10                          Light  \n",
       "422       300.0  NaN                   super 666666  \n",
       "\n",
       "[423 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "# corpus_root = 'drive/My Drive/Colab Notebooks/data science and machine learning/Boy or Girl/'\n",
    "# raw_data = pd.read_csv(corpus_root+'boy or girl 2024 train_missingValue.csv')\n",
    "\n",
    "raw_data = pd.read_csv(\"boy or girl 2024 train_missingValue.csv\")\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>self_intro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Beautiful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Enjoying being who I'm notsss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Practice Makes perfect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Straightforward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Humorous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>419</td>\n",
       "      <td>1</td>\n",
       "      <td>I hope i am a super hero.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>420</td>\n",
       "      <td>1</td>\n",
       "      <td>God damn dope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>421</td>\n",
       "      <td>1</td>\n",
       "      <td>Sunny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>422</td>\n",
       "      <td>2</td>\n",
       "      <td>Light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>423</td>\n",
       "      <td>1</td>\n",
       "      <td>super 666666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>423 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  gender                     self_intro\n",
       "0      1       2                      Beautiful\n",
       "1      2       2  Enjoying being who I'm notsss\n",
       "2      3       1         Practice Makes perfect\n",
       "3      4       1                Straightforward\n",
       "4      5       2                       Humorous\n",
       "..   ...     ...                            ...\n",
       "418  419       1      I hope i am a super hero.\n",
       "419  420       1                  God damn dope\n",
       "420  421       1                          Sunny\n",
       "421  422       2                          Light\n",
       "422  423       1                   super 666666\n",
       "\n",
       "[423 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_intro_data = pd.read_csv(\"gender_self_intro.csv\")\n",
    "self_intro_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gc5abxLSAJXP",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>star_sign</th>\n",
       "      <th>phone_os</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>sleepiness</th>\n",
       "      <th>iq</th>\n",
       "      <th>fb_friends</th>\n",
       "      <th>yt</th>\n",
       "      <th>self_intro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>å¤©è åº§</td>\n",
       "      <td>Apple</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>87.000</td>\n",
       "      <td>87.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>GOod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>é‡‘ç‰›åº§</td>\n",
       "      <td>Apple</td>\n",
       "      <td>175.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130.000</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Easygoing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>é›™å­åº§</td>\n",
       "      <td>Apple</td>\n",
       "      <td>155.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>150.000</td>\n",
       "      <td>400.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>I LOVE INTEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>è™•å¥³åº§</td>\n",
       "      <td>Apple</td>\n",
       "      <td>173.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>I'm a hard-work man, just do my best to finish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>å°„æ‰‹åº§</td>\n",
       "      <td>Android</td>\n",
       "      <td>164.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>130.000</td>\n",
       "      <td>505.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I'm smart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>391</td>\n",
       "      <td>0</td>\n",
       "      <td>è™•å¥³åº§</td>\n",
       "      <td>Android</td>\n",
       "      <td>160.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>75.000</td>\n",
       "      <td>98.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Starting by Starting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>392</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apple</td>\n",
       "      <td>170.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105.000</td>\n",
       "      <td>510.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A little bit smart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>393</td>\n",
       "      <td>0</td>\n",
       "      <td>é‡‘ç‰›åº§</td>\n",
       "      <td>Apple</td>\n",
       "      <td>160.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>600.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>Hi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>394</td>\n",
       "      <td>0</td>\n",
       "      <td>å·¨èŸ¹åº§</td>\n",
       "      <td>NaN</td>\n",
       "      <td>180.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>199.999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.0</td>\n",
       "      <td>I'm not beautiful, but smart ğŸ˜€</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>395</td>\n",
       "      <td>0</td>\n",
       "      <td>å°„æ‰‹åº§</td>\n",
       "      <td>Apple</td>\n",
       "      <td>175.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>135.000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>Like the shadow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>395 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  gender star_sign phone_os  height  weight  sleepiness       iq  \\\n",
       "0      1       0       å¤©è åº§    Apple     NaN   100.0         1.0   87.000   \n",
       "1      2       0       é‡‘ç‰›åº§    Apple   175.0    80.0         3.0  130.000   \n",
       "2      3       0       é›™å­åº§    Apple   155.0    45.0         3.0  150.000   \n",
       "3      4       0       è™•å¥³åº§    Apple   173.0    85.0         4.0  100.000   \n",
       "4      5       0       å°„æ‰‹åº§  Android   164.0    57.0         4.0  130.000   \n",
       "..   ...     ...       ...      ...     ...     ...         ...      ...   \n",
       "390  391       0       è™•å¥³åº§  Android   160.0    48.0         3.0   75.000   \n",
       "391  392       0       NaN    Apple   170.0     NaN         NaN  105.000   \n",
       "392  393       0       é‡‘ç‰›åº§    Apple   160.0    45.0         4.0  100.000   \n",
       "393  394       0       å·¨èŸ¹åº§      NaN   180.0     NaN         NaN  199.999   \n",
       "394  395       0       å°„æ‰‹åº§    Apple   175.0    69.0         5.0  135.000   \n",
       "\n",
       "     fb_friends      yt                                         self_intro  \n",
       "0          87.0    87.0                                               GOod  \n",
       "1        2000.0    30.0                                          Easygoing  \n",
       "2         400.0     9.0                                       I LOVE INTEL  \n",
       "3        2000.0    15.0  I'm a hard-work man, just do my best to finish...  \n",
       "4         505.0     2.0                                          I'm smart  \n",
       "..          ...     ...                                                ...  \n",
       "390        98.0     2.0                               Starting by Starting  \n",
       "391       510.0     NaN                                 A little bit smart  \n",
       "392       600.0  2000.0                                                 Hi  \n",
       "393         NaN    60.0                     I'm not beautiful, but smart ğŸ˜€  \n",
       "394       200.0  1400.0                                    Like the shadow  \n",
       "\n",
       "[395 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_data = pd.read_csv(corpus_root+'boy or girl 2024 test no ans_missingValue.csv')\n",
    "test_data = pd.read_csv(\"boy or girl 2024 test no ans_missingValue.csv\")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "def export_csv(df):\n",
    "  now = datetime.datetime.now().astimezone(pytz.timezone('Asia/Taipei'))\n",
    "  formatted_time = now.strftime('%Y-%m-%d %Hæ™‚%Måˆ†%Sç§’')\n",
    "  df.to_csv(formatted_time + \"predict.csv\", index=False,encoding=\"utf_8_sig\")\n",
    "  # df.to_csv(corpus_root + formatted_time + \"predict.csv\", index=False,encoding=\"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7agcKdAOQR1g"
   },
   "source": [
    "### å‰è™•ç† åˆªæ‰æ˜Ÿåº§å’Œæ‰‹æ©Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "\n",
    "# è¯»å–æ•°æ®\n",
    "data = raw_data\n",
    "\n",
    "# æ–‡æœ¬é¢„å¤„ç†å‡½æ•°\n",
    "def preprocess_text(text):\n",
    "    # åˆ†è¯\n",
    "    tokens = word_tokenize(text)\n",
    "    # å»é™¤æ ‡ç‚¹ç¬¦å·å’Œåœç”¨è¯\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stopwords.words('english')]\n",
    "    return tokens\n",
    "\n",
    "# å¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†\n",
    "data['tokens'] = data['self_intro'].apply(preprocess_text)\n",
    "\n",
    "# è®­ç»ƒ Word2Vec æ¨¡å‹\n",
    "model = Word2Vec(data['tokens'], vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# åˆ›å»º Word2Vec ç‰¹å¾å‡½æ•°\n",
    "def word2vec_feature(tokens):\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            vectors.append(model.wv[token])\n",
    "    if vectors:\n",
    "        return sum(vectors) / len(vectors)\n",
    "    else:\n",
    "        return [0] * model.vector_size\n",
    "\n",
    "# åº”ç”¨ Word2Vec ç‰¹å¾å‡½æ•°\n",
    "data['word2vec_feature'] = data['tokens'].apply(word2vec_feature)\n",
    "\n",
    "# å°†ç‰¹å¾æ·»åŠ åˆ°åŸå§‹æ•°æ®ä¸­\n",
    "data = pd.concat([data.drop(columns=['tokens']), pd.DataFrame(data['word2vec_feature'].tolist())], axis=1)\n",
    "\n",
    "# å°†æ•°æ®é›†åˆ†ä¸ºæ€§åˆ«ä¸º1å’Œæ€§åˆ«ä¸º2çš„ä¸¤ä¸ªå­æ•°æ®é›†\n",
    "gender1_data = data[data['gender'] == 1]\n",
    "gender2_data = data[data['gender'] == 2]\n",
    "\n",
    "# å¯ä»¥è¿›ä¸€æ­¥ä½¿ç”¨ gender1_data å’Œ gender2_data æ¥è®­ç»ƒåˆ†ç±»å™¨æˆ–è¿›è¡Œå…¶ä»–åˆ†æä»»åŠ¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# è®€å–è¨“ç·´è³‡æ–™é›†\n",
    "train_data = pd.read_csv(r\"..\\KNN\\dataset\\KNN_without_outlier.csv\")\n",
    "\n",
    "# å°‡self_introæ¬„ä½å¾è¨“ç·´è³‡æ–™ä¸­ç§»é™¤ï¼Œå› ç‚ºé€™è£¡ä¸æ‰“ç®—ä½¿ç”¨è©²æ¬„ä½ä½œç‚ºç‰¹å¾µ\n",
    "train_data = train_data.drop(columns=['self_intro'])\n",
    "\n",
    "# å°‡æ€§åˆ¥æ¨™ç±¤è¨­ç½®ç‚º0å’Œ1ï¼Œå…¶ä¸­1ä»£è¡¨ç”·æ€§ï¼Œ2ä»£è¡¨å¥³æ€§\n",
    "train_data['gender'] = train_data['gender'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "# å°‡è³‡æ–™é›†åˆ†ç‚ºç‰¹å¾µï¼ˆXï¼‰å’Œæ¨™ç±¤ï¼ˆyï¼‰\n",
    "X = train_data.drop(columns=['gender'])\n",
    "y = train_data['gender']\n",
    "\n",
    "# å°‡è³‡æ–™é›†åˆ†ç‚ºè¨“ç·´é›†å’Œé©—è­‰é›†\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# åˆå§‹åŒ–æ¨™æº–åŒ–å™¨\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# å°è¨“ç·´é›†å’Œé©—è­‰é›†é€²è¡Œæ¨™æº–åŒ–\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# åˆå§‹åŒ–éš¨æ©Ÿæ£®æ—åˆ†é¡å™¨\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# å®šç¾©è¶…åƒæ•¸ç¯„åœ\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# ä½¿ç”¨ RapidSearch (GridSearchCV) é€²è¡Œè¶…åƒæ•¸å„ªåŒ–\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# ç²å–æœ€ä½³æ¨¡å‹\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# åœ¨é©—è­‰é›†ä¸Šé€²è¡Œé æ¸¬\n",
    "val_predictions = best_rf.predict(X_val_scaled)\n",
    "\n",
    "# è¨ˆç®—æ¨¡å‹åœ¨é©—è­‰é›†ä¸Šçš„æº–ç¢ºç‡\n",
    "accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(\"Validation Accuracy:\", accuracy)\n",
    "\n",
    "# é€²è¡Œæ¸¬è©¦è³‡æ–™é›†çš„é æ¸¬\n",
    "test_data = pd.read_csv(r\"..\\KNN\\dataset\\test_KNN_without_outlier.csv\")\n",
    "test_data = test_data.drop(columns=['self_intro', 'id', 'gender'])\n",
    "test_data_scaled = scaler.transform(test_data)\n",
    "test_predictions = best_rf.predict(test_data_scaled)\n",
    "\n",
    "# å»ºç«‹æ–°çš„ DataFrame ä¾†å­˜æ”¾é æ¸¬çµæœ\n",
    "result_df = pd.DataFrame({'ID': range(1, len(test_predictions) + 1), 'gender': [2 if pred == 0 else pred for pred in test_predictions]})\n",
    "\n",
    "# å°‡çµæœå­˜å…¥æ–°çš„ CSV æª”æ¡ˆä¸­\n",
    "result_df.to_csv('prediction_result_RF_rapid.csv', index=False)\n",
    "\n",
    "# è¼¸å‡ºé æ¸¬çµæœ\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# è¯»å–æ•°æ®\n",
    "data = raw_data\n",
    "\n",
    "# æ–‡æœ¬é¢„å¤„ç†å‡½æ•°\n",
    "def preprocess_text(text):\n",
    "    # åˆ†è¯\n",
    "    tokens = word_tokenize(text)\n",
    "    # å»é™¤æ ‡ç‚¹ç¬¦å·å’Œåœç”¨è¯\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stopwords.words('english')]\n",
    "    return tokens\n",
    "\n",
    "# å¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†\n",
    "data['tokens'] = data['self_intro'].apply(preprocess_text)\n",
    "\n",
    "# è®­ç»ƒ Word2Vec æ¨¡å‹\n",
    "model = Word2Vec(data['tokens'], vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# åˆ›å»º Word2Vec ç‰¹å¾å‡½æ•°\n",
    "def word2vec_feature(tokens):\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            vectors.append(model.wv[token])\n",
    "    if vectors:\n",
    "        return sum(vectors) / len(vectors)\n",
    "    else:\n",
    "        return [0] * model.vector_size\n",
    "\n",
    "# åº”ç”¨ Word2Vec ç‰¹å¾å‡½æ•°\n",
    "data['word2vec_feature'] = data['tokens'].apply(word2vec_feature)\n",
    "\n",
    "# å°†ç‰¹å¾æ·»åŠ åˆ°åŸå§‹æ•°æ®ä¸­\n",
    "data = pd.concat([data.drop(columns=['tokens']), pd.DataFrame(data['word2vec_feature'].tolist())], axis=1)\n",
    "\n",
    "# å°†æ•°æ®é›†åˆ†ä¸ºæ€§åˆ«ä¸º1å’Œæ€§åˆ«ä¸º2çš„ä¸¤ä¸ªå­æ•°æ®é›†\n",
    "gender1_data = data[data['gender'] == 1]\n",
    "gender2_data = data[data['gender'] == 2]\n",
    "\n",
    "# å¯ä»¥è¿›ä¸€æ­¥ä½¿ç”¨ gender1_data å’Œ gender2_data æ¥è®­ç»ƒåˆ†ç±»å™¨æˆ–è¿›è¡Œå…¶ä»–åˆ†æä»»åŠ¡\n",
    "\n",
    "# å°†self_introæ¬„ä½å¾è¨“ç·´è³‡æ–™ä¸­ç§»é™¤ï¼Œå› ç‚ºé€™è£¡ä¸æ‰“ç®—ä½¿ç”¨è©²æ¬„ä½ä½œç‚ºç‰¹å¾µ\n",
    "train_data = pd.read_csv(r\"..\\KNN\\dataset\\KNN_without_outlier.csv\")\n",
    "train_data = train_data.drop(columns=['self_intro'])\n",
    "\n",
    "# å°†æ€§åˆ«æ ‡ç­¾è®¾ç½®ç‚º0å’Œ1ï¼Œå…¶ä¸­1ä»£è¡¨ç”·æ€§ï¼Œ2ä»£è¡¨å¥³æ€§\n",
    "train_data['gender'] = train_data['gender'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "# å°†è³‡æ–™é›†åˆ†ä¸ºç‰¹å¾µï¼ˆXï¼‰å’Œæ ‡ç­¾ï¼ˆyï¼‰\n",
    "X = train_data.drop(columns=['gender'])\n",
    "y = train_data['gender']\n",
    "\n",
    "# å°†è³‡æ–™é›†åˆ†ä¸ºè¨“ç·´é›†å’Œé©—è­‰é›†\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# åˆå§‹åŒ–æ ‡å‡†åŒ–å™¨\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# å¯¹è¨“ç·´é›†å’Œé©—è­‰é›†é€²è¡Œæ¨™æº–åŒ–\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# åˆå§‹åŒ–éšæœºæ£®æ—åˆ†é¡å™¨\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# å®šä¹‰è¶…å‚æ•°èŒƒå›´\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# ä½¿ç”¨ RapidSearch (GridSearchCV) é€²è¡Œè¶…å‚æ•°ä¼˜åŒ–\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# è·å–æœ€ä½³æ¨¡å‹\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# åœ¨é©—è­‰é›†ä¸Šé€²è¡Œé æ¸¬\n",
    "val_predictions = best_rf.predict(X_val_scaled)\n",
    "\n",
    "# è¨ˆç®—æ¨¡å‹åœ¨é©—è­‰é›†ä¸Šçš„æº–ç¢ºç‡\n",
    "accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(\"Validation Accuracy:\", accuracy)\n",
    "\n",
    "# è¿›è¡Œæµ‹è¯•è³‡æ–™é›†çš„é¢„æµ‹\n",
    "test_data = pd.read_csv(r\"..\\KNN\\dataset\\test_KNN_without_outlier.csv\")\n",
    "test_data = test_data.drop(columns=['self_intro', 'id', 'gender'])\n",
    "test_data_scaled = scaler.transform(test_data)\n",
    "test_predictions = best_rf.predict(test_data_scaled)\n",
    "\n",
    "# å»ºç«‹æ–°çš„ DataFrame ä¾†å­˜æ”¾é æ¸¬çµæœ\n",
    "result_df = pd.DataFrame({'ID': range(1, len(test_predictions) + 1), 'gender': [2 if pred == 0 else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.13288987e-03, -4.45661834e-03, -1.06700230e-03, ...,\n",
       "         1.50306921e+00, -5.85793799e-02, -1.38824952e-01],\n",
       "       [-2.37336010e-03,  3.95035231e-03,  3.54133267e-03, ...,\n",
       "         9.45725371e-02, -5.89446858e-02, -1.34398690e-01],\n",
       "       [-2.25528912e-03,  3.27215041e-03, -4.91253508e-04, ...,\n",
       "        -1.03222480e+00, -5.86652168e-02, -1.32501721e-01],\n",
       "       ...,\n",
       "       [ 5.55166230e-03, -3.48776113e-04, -9.96322487e-04, ...,\n",
       "        -1.11673460e+00, -5.71480992e-02, -1.36295659e-01],\n",
       "       [ 2.09900143e-04,  3.22359335e-03, -1.63235550e-03, ...,\n",
       "         1.50306921e+00, -5.87790006e-02, -1.26178490e-01],\n",
       "       [ 4.79276711e-03, -3.63464421e-03, -4.25703824e-03, ...,\n",
       "        -1.70830320e+00, -5.91443066e-02, -1.37231498e-01]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import numpy as np\n",
    "\n",
    "train_data = pd.read_csv(r\"..\\KNN\\dataset\\KNN_without_outlier.csv\")\n",
    "train_text_data = train_data.copy()\n",
    "\n",
    "# æ–‡æœ¬é¢„å¤„ç†å‡½æ•°\n",
    "def preprocess_text(text):\n",
    "    # åˆ†è¯\n",
    "    tokens = word_tokenize(text)\n",
    "    # å»é™¤æ ‡ç‚¹ç¬¦å·å’Œåœç”¨è¯\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stopwords.words('english')]\n",
    "    return tokens\n",
    "\n",
    "# å¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†\n",
    "train_text_data['tokens'] = train_text_data['self_intro'].apply(preprocess_text)\n",
    "\n",
    "# è®­ç»ƒ Word2Vec æ¨¡å‹\n",
    "model = Word2Vec(train_text_data['tokens'], vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# åˆ›å»º Word2Vec ç‰¹å¾å‡½æ•°\n",
    "def word2vec_feature(tokens):\n",
    "    vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "    if vectors:\n",
    "        return sum(vectors) / len(vectors)\n",
    "    else:\n",
    "        return [0] * model.vector_size\n",
    "\n",
    "# åº”ç”¨ Word2Vec ç‰¹å¾å‡½æ•°\n",
    "train_text_data['word2vec_feature'] = train_text_data['tokens'].apply(word2vec_feature)\n",
    "\n",
    "\n",
    "# ä»åŸå§‹æ•°æ®ä¸­ç§»é™¤'tokens'å’Œ'self_intro'åˆ—\n",
    "X_text_features = pd.DataFrame(train_text_data['word2vec_feature'].tolist())\n",
    "\n",
    "# ä»åŸå§‹æ•°æ®ä¸­ç§»é™¤'tokens'å’Œ'self_intro'åˆ—\n",
    "train_data = train_data.drop(columns=['self_intro','gender'])\n",
    "\n",
    "# æ ‡å‡†åŒ–éæ–‡æœ¬ç‰¹å¾\n",
    "scaler = StandardScaler()\n",
    "X_non_text_features = scaler.fit_transform(train_data)\n",
    "\n",
    "# å°†æ–‡æœ¬ç‰¹å¾å’Œéæ–‡æœ¬ç‰¹å¾ç»“åˆ\n",
    "X = np.hstack((X_text_features, X_non_text_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8674698795180723\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "train_data = pd.read_csv(r\"..\\KNN\\dataset\\KNN_without_outlier.csv\")\n",
    "train_text_data = train_data.copy()\n",
    "\n",
    "# æ–‡æœ¬é¢„å¤„ç†å‡½æ•°\n",
    "def preprocess_text(text):\n",
    "    # åˆ†è¯\n",
    "    tokens = word_tokenize(text)\n",
    "    # å»é™¤æ ‡ç‚¹ç¬¦å·å’Œåœç”¨è¯\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stopwords.words('english')]\n",
    "    return tokens\n",
    "\n",
    "# å¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†\n",
    "train_text_data['tokens'] = train_text_data['self_intro'].apply(preprocess_text)\n",
    "\n",
    "# è®­ç»ƒ Word2Vec æ¨¡å‹\n",
    "model = Word2Vec(train_text_data['tokens'], vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# åˆ›å»º Word2Vec ç‰¹å¾å‡½æ•°\n",
    "def word2vec_feature(tokens):\n",
    "    vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "    if vectors:\n",
    "        return sum(vectors) / len(vectors)\n",
    "    else:\n",
    "        return [0] * model.vector_size\n",
    "\n",
    "# åº”ç”¨ Word2Vec ç‰¹å¾å‡½æ•°\n",
    "train_text_data['word2vec_feature'] = train_text_data['tokens'].apply(word2vec_feature)\n",
    "\n",
    "\n",
    "# ä»åŸå§‹æ•°æ®ä¸­ç§»é™¤'tokens'å’Œ'self_intro'åˆ—\n",
    "X_text_features = pd.DataFrame(train_text_data['word2vec_feature'].tolist())\n",
    "\n",
    "y = train_data['gender']\n",
    "\n",
    "# ä»åŸå§‹æ•°æ®ä¸­ç§»é™¤'tokens'å’Œ'self_intro'åˆ—\n",
    "train_data = train_data.drop(columns=['self_intro','gender'])\n",
    "\n",
    "# æ ‡å‡†åŒ–éæ–‡æœ¬ç‰¹å¾\n",
    "scaler = StandardScaler()\n",
    "X_non_text_features = scaler.fit_transform(train_data)\n",
    "\n",
    "# å°†æ–‡æœ¬ç‰¹å¾å’Œéæ–‡æœ¬ç‰¹å¾ç»“åˆ\n",
    "X = np.hstack((X_text_features, X_non_text_features))\n",
    "\n",
    "# åˆ†å‰²æ•°æ®é›†\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# åˆå§‹åŒ–å’Œè®­ç»ƒéšæœºæ£®æ—åˆ†ç±»å™¨\n",
    "# æ‚¨å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´è¿™äº›å‚æ•°\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# åœ¨éªŒè¯é›†ä¸Šé¢„æµ‹å¹¶è®¡ç®—å‡†ç¡®ç‡\n",
    "y_pred = rf.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Validation Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GOEsoqyQOqk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBE4tmbJQOiD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CA24qYmQQOVz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "WtBskiyxCciN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6eGWsqQPCcc-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOHal7C7CcTt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOhbIoqpSLLjGY0V81qNjFU",
   "collapsed_sections": [
    "EK_th23ovb9u",
    "WwAqev3X9xM_",
    "gtdDQR2k96mb"
   ],
   "provenance": [
    {
     "file_id": "14FXq3PtDMQOJkzyRoYm87rjUxq6tMzP1",
     "timestamp": 1711607688964
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
