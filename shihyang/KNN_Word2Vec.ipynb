{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "B_QiqBOzvGra"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>star_sign</th>\n",
       "      <th>phone_os</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>sleepiness</th>\n",
       "      <th>iq</th>\n",
       "      <th>fb_friends</th>\n",
       "      <th>yt</th>\n",
       "      <th>self_intro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>處女座</td>\n",
       "      <td>Apple</td>\n",
       "      <td>154.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>583.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Beautiful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>處女座</td>\n",
       "      <td>NaN</td>\n",
       "      <td>156.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Enjoying being who I'm notsss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>射手座</td>\n",
       "      <td>Android</td>\n",
       "      <td>170.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Practice Makes perfect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>射手座</td>\n",
       "      <td>Apple</td>\n",
       "      <td>170.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Straightforward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>射手座</td>\n",
       "      <td>Android</td>\n",
       "      <td>158.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Humorous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>419</td>\n",
       "      <td>1</td>\n",
       "      <td>處女座</td>\n",
       "      <td>Android</td>\n",
       "      <td>166.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>I hope i am a super hero.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>420</td>\n",
       "      <td>1</td>\n",
       "      <td>牡羊座</td>\n",
       "      <td>Android</td>\n",
       "      <td>176.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>2</td>\n",
       "      <td>God damn dope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>421</td>\n",
       "      <td>1</td>\n",
       "      <td>天秤座</td>\n",
       "      <td>Apple</td>\n",
       "      <td>174.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sunny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>422</td>\n",
       "      <td>2</td>\n",
       "      <td>天蠍座</td>\n",
       "      <td>Apple</td>\n",
       "      <td>167.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>483.0</td>\n",
       "      <td>10</td>\n",
       "      <td>Light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>423</td>\n",
       "      <td>1</td>\n",
       "      <td>雙魚座</td>\n",
       "      <td>Android</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>super 666666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>423 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  gender star_sign phone_os  height  weight  sleepiness     iq  \\\n",
       "0      1       2       處女座    Apple   154.0    43.0         2.0  180.0   \n",
       "1      2       2       處女座      NaN   156.0    47.0         2.0  130.0   \n",
       "2      3       1       射手座  Android   170.0    61.0         3.0   90.0   \n",
       "3      4       1       射手座    Apple   170.0    62.0         4.0  100.0   \n",
       "4      5       2       射手座  Android   158.0     NaN         3.0    NaN   \n",
       "..   ...     ...       ...      ...     ...     ...         ...    ...   \n",
       "418  419       1       處女座  Android   166.0    66.0         4.0   90.0   \n",
       "419  420       1       牡羊座  Android   176.0    65.0         4.0   87.0   \n",
       "420  421       1       天秤座    Apple   174.0    72.0         NaN    NaN   \n",
       "421  422       2       天蠍座    Apple   167.0    50.0         3.0  180.0   \n",
       "422  423       1       雙魚座  Android     NaN    68.0         3.0   66.0   \n",
       "\n",
       "     fb_friends   yt                     self_intro  \n",
       "0         583.0    0                      Beautiful  \n",
       "1         400.0  3.5  Enjoying being who I'm notsss  \n",
       "2         540.0    5         Practice Makes perfect  \n",
       "3         173.0    5                Straightforward  \n",
       "4           NaN  1.2                       Humorous  \n",
       "..          ...  ...                            ...  \n",
       "418      1000.0    1      I hope i am a super hero.  \n",
       "419      1300.0    2                  God damn dope  \n",
       "420         NaN  NaN                          Sunny  \n",
       "421       483.0   10                          Light  \n",
       "422       300.0  NaN                   super 666666  \n",
       "\n",
       "[423 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "# corpus_root = 'drive/My Drive/Colab Notebooks/data science and machine learning/Boy or Girl/'\n",
    "# raw_data = pd.read_csv(corpus_root+'boy or girl 2024 train_missingValue.csv')\n",
    "\n",
    "raw_data = pd.read_csv(\"boy or girl 2024 train_missingValue.csv\")\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>self_intro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Beautiful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Enjoying being who I'm notsss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Practice Makes perfect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Straightforward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Humorous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>419</td>\n",
       "      <td>1</td>\n",
       "      <td>I hope i am a super hero.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>420</td>\n",
       "      <td>1</td>\n",
       "      <td>God damn dope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>421</td>\n",
       "      <td>1</td>\n",
       "      <td>Sunny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>422</td>\n",
       "      <td>2</td>\n",
       "      <td>Light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>423</td>\n",
       "      <td>1</td>\n",
       "      <td>super 666666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>423 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  gender                     self_intro\n",
       "0      1       2                      Beautiful\n",
       "1      2       2  Enjoying being who I'm notsss\n",
       "2      3       1         Practice Makes perfect\n",
       "3      4       1                Straightforward\n",
       "4      5       2                       Humorous\n",
       "..   ...     ...                            ...\n",
       "418  419       1      I hope i am a super hero.\n",
       "419  420       1                  God damn dope\n",
       "420  421       1                          Sunny\n",
       "421  422       2                          Light\n",
       "422  423       1                   super 666666\n",
       "\n",
       "[423 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_intro_data = pd.read_csv(\"gender_self_intro.csv\")\n",
    "self_intro_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gc5abxLSAJXP",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>star_sign</th>\n",
       "      <th>phone_os</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>sleepiness</th>\n",
       "      <th>iq</th>\n",
       "      <th>fb_friends</th>\n",
       "      <th>yt</th>\n",
       "      <th>self_intro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>天蠍座</td>\n",
       "      <td>Apple</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>87.000</td>\n",
       "      <td>87.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>GOod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>金牛座</td>\n",
       "      <td>Apple</td>\n",
       "      <td>175.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130.000</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Easygoing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>雙子座</td>\n",
       "      <td>Apple</td>\n",
       "      <td>155.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>150.000</td>\n",
       "      <td>400.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>I LOVE INTEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>處女座</td>\n",
       "      <td>Apple</td>\n",
       "      <td>173.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>I'm a hard-work man, just do my best to finish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>射手座</td>\n",
       "      <td>Android</td>\n",
       "      <td>164.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>130.000</td>\n",
       "      <td>505.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I'm smart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>391</td>\n",
       "      <td>0</td>\n",
       "      <td>處女座</td>\n",
       "      <td>Android</td>\n",
       "      <td>160.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>75.000</td>\n",
       "      <td>98.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Starting by Starting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>392</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apple</td>\n",
       "      <td>170.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105.000</td>\n",
       "      <td>510.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A little bit smart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>393</td>\n",
       "      <td>0</td>\n",
       "      <td>金牛座</td>\n",
       "      <td>Apple</td>\n",
       "      <td>160.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>600.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>Hi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>394</td>\n",
       "      <td>0</td>\n",
       "      <td>巨蟹座</td>\n",
       "      <td>NaN</td>\n",
       "      <td>180.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>199.999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.0</td>\n",
       "      <td>I'm not beautiful, but smart 😀</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>395</td>\n",
       "      <td>0</td>\n",
       "      <td>射手座</td>\n",
       "      <td>Apple</td>\n",
       "      <td>175.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>135.000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>Like the shadow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>395 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  gender star_sign phone_os  height  weight  sleepiness       iq  \\\n",
       "0      1       0       天蠍座    Apple     NaN   100.0         1.0   87.000   \n",
       "1      2       0       金牛座    Apple   175.0    80.0         3.0  130.000   \n",
       "2      3       0       雙子座    Apple   155.0    45.0         3.0  150.000   \n",
       "3      4       0       處女座    Apple   173.0    85.0         4.0  100.000   \n",
       "4      5       0       射手座  Android   164.0    57.0         4.0  130.000   \n",
       "..   ...     ...       ...      ...     ...     ...         ...      ...   \n",
       "390  391       0       處女座  Android   160.0    48.0         3.0   75.000   \n",
       "391  392       0       NaN    Apple   170.0     NaN         NaN  105.000   \n",
       "392  393       0       金牛座    Apple   160.0    45.0         4.0  100.000   \n",
       "393  394       0       巨蟹座      NaN   180.0     NaN         NaN  199.999   \n",
       "394  395       0       射手座    Apple   175.0    69.0         5.0  135.000   \n",
       "\n",
       "     fb_friends      yt                                         self_intro  \n",
       "0          87.0    87.0                                               GOod  \n",
       "1        2000.0    30.0                                          Easygoing  \n",
       "2         400.0     9.0                                       I LOVE INTEL  \n",
       "3        2000.0    15.0  I'm a hard-work man, just do my best to finish...  \n",
       "4         505.0     2.0                                          I'm smart  \n",
       "..          ...     ...                                                ...  \n",
       "390        98.0     2.0                               Starting by Starting  \n",
       "391       510.0     NaN                                 A little bit smart  \n",
       "392       600.0  2000.0                                                 Hi  \n",
       "393         NaN    60.0                     I'm not beautiful, but smart 😀  \n",
       "394       200.0  1400.0                                    Like the shadow  \n",
       "\n",
       "[395 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_data = pd.read_csv(corpus_root+'boy or girl 2024 test no ans_missingValue.csv')\n",
    "test_data = pd.read_csv(\"boy or girl 2024 test no ans_missingValue.csv\")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "def export_csv(df):\n",
    "  now = datetime.datetime.now().astimezone(pytz.timezone('Asia/Taipei'))\n",
    "  formatted_time = now.strftime('%Y-%m-%d %H時%M分%S秒')\n",
    "  df.to_csv(formatted_time + \"predict.csv\", index=False,encoding=\"utf_8_sig\")\n",
    "  # df.to_csv(corpus_root + formatted_time + \"predict.csv\", index=False,encoding=\"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7agcKdAOQR1g"
   },
   "source": [
    "### 前處理 刪掉星座和手機"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "\n",
    "# 读取数据\n",
    "data = raw_data\n",
    "\n",
    "# 文本预处理函数\n",
    "def preprocess_text(text):\n",
    "    # 分词\n",
    "    tokens = word_tokenize(text)\n",
    "    # 去除标点符号和停用词\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stopwords.words('english')]\n",
    "    return tokens\n",
    "\n",
    "# 对文本进行预处理\n",
    "data['tokens'] = data['self_intro'].apply(preprocess_text)\n",
    "\n",
    "# 训练 Word2Vec 模型\n",
    "model = Word2Vec(data['tokens'], vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# 创建 Word2Vec 特征函数\n",
    "def word2vec_feature(tokens):\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            vectors.append(model.wv[token])\n",
    "    if vectors:\n",
    "        return sum(vectors) / len(vectors)\n",
    "    else:\n",
    "        return [0] * model.vector_size\n",
    "\n",
    "# 应用 Word2Vec 特征函数\n",
    "data['word2vec_feature'] = data['tokens'].apply(word2vec_feature)\n",
    "\n",
    "# 将特征添加到原始数据中\n",
    "data = pd.concat([data.drop(columns=['tokens']), pd.DataFrame(data['word2vec_feature'].tolist())], axis=1)\n",
    "\n",
    "# 将数据集分为性别为1和性别为2的两个子数据集\n",
    "gender1_data = data[data['gender'] == 1]\n",
    "gender2_data = data[data['gender'] == 2]\n",
    "\n",
    "# 可以进一步使用 gender1_data 和 gender2_data 来训练分类器或进行其他分析任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# 讀取訓練資料集\n",
    "train_data = pd.read_csv(r\"..\\KNN\\dataset\\KNN_without_outlier.csv\")\n",
    "\n",
    "# 將self_intro欄位從訓練資料中移除，因為這裡不打算使用該欄位作為特徵\n",
    "train_data = train_data.drop(columns=['self_intro'])\n",
    "\n",
    "# 將性別標籤設置為0和1，其中1代表男性，2代表女性\n",
    "train_data['gender'] = train_data['gender'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "# 將資料集分為特徵（X）和標籤（y）\n",
    "X = train_data.drop(columns=['gender'])\n",
    "y = train_data['gender']\n",
    "\n",
    "# 將資料集分為訓練集和驗證集\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 初始化標準化器\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 對訓練集和驗證集進行標準化\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# 初始化隨機森林分類器\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# 定義超參數範圍\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# 使用 RapidSearch (GridSearchCV) 進行超參數優化\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 獲取最佳模型\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# 在驗證集上進行預測\n",
    "val_predictions = best_rf.predict(X_val_scaled)\n",
    "\n",
    "# 計算模型在驗證集上的準確率\n",
    "accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(\"Validation Accuracy:\", accuracy)\n",
    "\n",
    "# 進行測試資料集的預測\n",
    "test_data = pd.read_csv(r\"..\\KNN\\dataset\\test_KNN_without_outlier.csv\")\n",
    "test_data = test_data.drop(columns=['self_intro', 'id', 'gender'])\n",
    "test_data_scaled = scaler.transform(test_data)\n",
    "test_predictions = best_rf.predict(test_data_scaled)\n",
    "\n",
    "# 建立新的 DataFrame 來存放預測結果\n",
    "result_df = pd.DataFrame({'ID': range(1, len(test_predictions) + 1), 'gender': [2 if pred == 0 else pred for pred in test_predictions]})\n",
    "\n",
    "# 將結果存入新的 CSV 檔案中\n",
    "result_df.to_csv('prediction_result_RF_rapid.csv', index=False)\n",
    "\n",
    "# 輸出預測結果\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# 读取数据\n",
    "data = raw_data\n",
    "\n",
    "# 文本预处理函数\n",
    "def preprocess_text(text):\n",
    "    # 分词\n",
    "    tokens = word_tokenize(text)\n",
    "    # 去除标点符号和停用词\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stopwords.words('english')]\n",
    "    return tokens\n",
    "\n",
    "# 对文本进行预处理\n",
    "data['tokens'] = data['self_intro'].apply(preprocess_text)\n",
    "\n",
    "# 训练 Word2Vec 模型\n",
    "model = Word2Vec(data['tokens'], vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# 创建 Word2Vec 特征函数\n",
    "def word2vec_feature(tokens):\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            vectors.append(model.wv[token])\n",
    "    if vectors:\n",
    "        return sum(vectors) / len(vectors)\n",
    "    else:\n",
    "        return [0] * model.vector_size\n",
    "\n",
    "# 应用 Word2Vec 特征函数\n",
    "data['word2vec_feature'] = data['tokens'].apply(word2vec_feature)\n",
    "\n",
    "# 将特征添加到原始数据中\n",
    "data = pd.concat([data.drop(columns=['tokens']), pd.DataFrame(data['word2vec_feature'].tolist())], axis=1)\n",
    "\n",
    "# 将数据集分为性别为1和性别为2的两个子数据集\n",
    "gender1_data = data[data['gender'] == 1]\n",
    "gender2_data = data[data['gender'] == 2]\n",
    "\n",
    "# 可以进一步使用 gender1_data 和 gender2_data 来训练分类器或进行其他分析任务\n",
    "\n",
    "# 将self_intro欄位從訓練資料中移除，因為這裡不打算使用該欄位作為特徵\n",
    "train_data = pd.read_csv(r\"..\\KNN\\dataset\\KNN_without_outlier.csv\")\n",
    "train_data = train_data.drop(columns=['self_intro'])\n",
    "\n",
    "# 将性别标签设置為0和1，其中1代表男性，2代表女性\n",
    "train_data['gender'] = train_data['gender'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "# 将資料集分为特徵（X）和标签（y）\n",
    "X = train_data.drop(columns=['gender'])\n",
    "y = train_data['gender']\n",
    "\n",
    "# 将資料集分为訓練集和驗證集\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 初始化标准化器\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 对訓練集和驗證集進行標準化\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# 初始化随机森林分類器\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# 定义超参数范围\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# 使用 RapidSearch (GridSearchCV) 進行超参数优化\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 获取最佳模型\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# 在驗證集上進行預測\n",
    "val_predictions = best_rf.predict(X_val_scaled)\n",
    "\n",
    "# 計算模型在驗證集上的準確率\n",
    "accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(\"Validation Accuracy:\", accuracy)\n",
    "\n",
    "# 进行测试資料集的预测\n",
    "test_data = pd.read_csv(r\"..\\KNN\\dataset\\test_KNN_without_outlier.csv\")\n",
    "test_data = test_data.drop(columns=['self_intro', 'id', 'gender'])\n",
    "test_data_scaled = scaler.transform(test_data)\n",
    "test_predictions = best_rf.predict(test_data_scaled)\n",
    "\n",
    "# 建立新的 DataFrame 來存放預測結果\n",
    "result_df = pd.DataFrame({'ID': range(1, len(test_predictions) + 1), 'gender': [2 if pred == 0 else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.13288987e-03, -4.45661834e-03, -1.06700230e-03, ...,\n",
       "         1.50306921e+00, -5.85793799e-02, -1.38824952e-01],\n",
       "       [-2.37336010e-03,  3.95035231e-03,  3.54133267e-03, ...,\n",
       "         9.45725371e-02, -5.89446858e-02, -1.34398690e-01],\n",
       "       [-2.25528912e-03,  3.27215041e-03, -4.91253508e-04, ...,\n",
       "        -1.03222480e+00, -5.86652168e-02, -1.32501721e-01],\n",
       "       ...,\n",
       "       [ 5.55166230e-03, -3.48776113e-04, -9.96322487e-04, ...,\n",
       "        -1.11673460e+00, -5.71480992e-02, -1.36295659e-01],\n",
       "       [ 2.09900143e-04,  3.22359335e-03, -1.63235550e-03, ...,\n",
       "         1.50306921e+00, -5.87790006e-02, -1.26178490e-01],\n",
       "       [ 4.79276711e-03, -3.63464421e-03, -4.25703824e-03, ...,\n",
       "        -1.70830320e+00, -5.91443066e-02, -1.37231498e-01]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import numpy as np\n",
    "\n",
    "train_data = pd.read_csv(r\"..\\KNN\\dataset\\KNN_without_outlier.csv\")\n",
    "train_text_data = train_data.copy()\n",
    "\n",
    "# 文本预处理函数\n",
    "def preprocess_text(text):\n",
    "    # 分词\n",
    "    tokens = word_tokenize(text)\n",
    "    # 去除标点符号和停用词\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stopwords.words('english')]\n",
    "    return tokens\n",
    "\n",
    "# 对文本进行预处理\n",
    "train_text_data['tokens'] = train_text_data['self_intro'].apply(preprocess_text)\n",
    "\n",
    "# 训练 Word2Vec 模型\n",
    "model = Word2Vec(train_text_data['tokens'], vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# 创建 Word2Vec 特征函数\n",
    "def word2vec_feature(tokens):\n",
    "    vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "    if vectors:\n",
    "        return sum(vectors) / len(vectors)\n",
    "    else:\n",
    "        return [0] * model.vector_size\n",
    "\n",
    "# 应用 Word2Vec 特征函数\n",
    "train_text_data['word2vec_feature'] = train_text_data['tokens'].apply(word2vec_feature)\n",
    "\n",
    "\n",
    "# 从原始数据中移除'tokens'和'self_intro'列\n",
    "X_text_features = pd.DataFrame(train_text_data['word2vec_feature'].tolist())\n",
    "\n",
    "# 从原始数据中移除'tokens'和'self_intro'列\n",
    "train_data = train_data.drop(columns=['self_intro','gender'])\n",
    "\n",
    "# 标准化非文本特征\n",
    "scaler = StandardScaler()\n",
    "X_non_text_features = scaler.fit_transform(train_data)\n",
    "\n",
    "# 将文本特征和非文本特征结合\n",
    "X = np.hstack((X_text_features, X_non_text_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8674698795180723\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "train_data = pd.read_csv(r\"..\\KNN\\dataset\\KNN_without_outlier.csv\")\n",
    "train_text_data = train_data.copy()\n",
    "\n",
    "# 文本预处理函数\n",
    "def preprocess_text(text):\n",
    "    # 分词\n",
    "    tokens = word_tokenize(text)\n",
    "    # 去除标点符号和停用词\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stopwords.words('english')]\n",
    "    return tokens\n",
    "\n",
    "# 对文本进行预处理\n",
    "train_text_data['tokens'] = train_text_data['self_intro'].apply(preprocess_text)\n",
    "\n",
    "# 训练 Word2Vec 模型\n",
    "model = Word2Vec(train_text_data['tokens'], vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# 创建 Word2Vec 特征函数\n",
    "def word2vec_feature(tokens):\n",
    "    vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "    if vectors:\n",
    "        return sum(vectors) / len(vectors)\n",
    "    else:\n",
    "        return [0] * model.vector_size\n",
    "\n",
    "# 应用 Word2Vec 特征函数\n",
    "train_text_data['word2vec_feature'] = train_text_data['tokens'].apply(word2vec_feature)\n",
    "\n",
    "\n",
    "# 从原始数据中移除'tokens'和'self_intro'列\n",
    "X_text_features = pd.DataFrame(train_text_data['word2vec_feature'].tolist())\n",
    "\n",
    "y = train_data['gender']\n",
    "\n",
    "# 从原始数据中移除'tokens'和'self_intro'列\n",
    "train_data = train_data.drop(columns=['self_intro','gender'])\n",
    "\n",
    "# 标准化非文本特征\n",
    "scaler = StandardScaler()\n",
    "X_non_text_features = scaler.fit_transform(train_data)\n",
    "\n",
    "# 将文本特征和非文本特征结合\n",
    "X = np.hstack((X_text_features, X_non_text_features))\n",
    "\n",
    "# 分割数据集\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 初始化和训练随机森林分类器\n",
    "# 您可以根据需要调整这些参数\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# 在验证集上预测并计算准确率\n",
    "y_pred = rf.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Validation Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GOEsoqyQOqk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBE4tmbJQOiD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CA24qYmQQOVz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "WtBskiyxCciN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6eGWsqQPCcc-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOHal7C7CcTt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOhbIoqpSLLjGY0V81qNjFU",
   "collapsed_sections": [
    "EK_th23ovb9u",
    "WwAqev3X9xM_",
    "gtdDQR2k96mb"
   ],
   "provenance": [
    {
     "file_id": "14FXq3PtDMQOJkzyRoYm87rjUxq6tMzP1",
     "timestamp": 1711607688964
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
